{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNv1OQHE/zesYGKF2txWZgs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### California_housing 데이터셋으로 아래사항을 참조하여 주택가격을 예측하는 회귀모델을 개발하세요.\n","\n","- 전체 회귀모델을 적용\n","- 각 모델별 최적 하이퍼파라미터 - GridSearchCV 활용\n","- 평가지수 MSE 기준으로 가장 성능이 좋은 모델과 파라미터를 적용하여 평가 결과를 출력"],"metadata":{"id":"AX177OLEXY01"}},{"cell_type":"code","source":["# Desicion Tree v\n","# Random Forest v\n","# Logistic Regression v\n","# KNN Argorism\n","# Support Vector Machines ( SVC - Linear ) v\n","# KNeighborClassifiers\n","# Gradient Boosting Regression v\n","# LightGBM v\n","# Ridge Regression v\n","# Lasso Regression v\n","# Elastic Net Regression v"],"metadata":{"id":"QYNOKtdvZgs6","executionInfo":{"status":"ok","timestamp":1722403507384,"user_tz":-540,"elapsed":352,"user":{"displayName":"이현석","userId":"06775013589116224529"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n","\n","# 데이터 로드\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","\n","# 데이터 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 파이프라인 구성\n","pipeline = Pipeline([\n","    ('svd', TruncatedSVD(n_components=10)),\n","    ('logreg', LogisticRegression(max_iter=1000))\n","])\n","\n","# 하이퍼파라미터 그리드 설정\n","param_grid = {\n","    'svd__n_components': [2, 5, 10],\n","    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100]\n","}\n","\n","# GridSearchCV를 사용한 하이퍼파라미터 튜닝\n","grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","\n","# 최적의 하이퍼파라미터 출력\n","print(\"Best Parameters:\", grid_search.best_params_)\n","\n","# 평가 사용자 함수 정의\n","def evaluate_model(model, X_test, y_test):\n","    # 예측\n","    y_pred = model.predict(X_test)\n","\n","    # 정확도\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    # 분류 보고서 생성\n","    report = classification_report(y_test, y_pred)\n","\n","    # ROC AUC 점수 계산\n","    y_pred_proba = model.predict_proba(X_test)[:, 1]\n","    roc_auc = roc_auc_score(y_test, y_pred_proba)\n","\n","    # 결과 출력\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Classification Report:\\n\", report)\n","    print(\"ROC AUC Score:\", roc_auc)\n","\n","# 최적의 모델을 사용하여 테스트 데이터 평가\n","evaluate_model(grid_search.best_estimator_, X_test, y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"dhIKPOmsOlHp","executionInfo":{"status":"ok","timestamp":1722403515933,"user_tz":-540,"elapsed":4993,"user":{"displayName":"이현석","userId":"06775013589116224529"}},"outputId":"f877a8a7-7e09-44ca-ce1f-9328e071e522"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["Best Parameters: {'logreg__C': 10, 'svd__n_components': 10}\n","Accuracy: 0.956140350877193\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.97      0.91      0.94        43\n","           1       0.95      0.99      0.97        71\n","\n","    accuracy                           0.96       114\n","   macro avg       0.96      0.95      0.95       114\n","weighted avg       0.96      0.96      0.96       114\n","\n","ROC AUC Score: 0.9950867998689813\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]},{"cell_type":"code","source":["from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","data = fetch_california_housing()\n","\n","# 데이터 로드 및 분할\n","X = data.data\n","y = data.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 데이터 스케일링\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"],"metadata":{"id":"MdIPCLAmgoJp","executionInfo":{"status":"ok","timestamp":1722407882551,"user_tz":-540,"elapsed":366,"user":{"displayName":"이현석","userId":"06775013589116224529"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from sklearn.model_selection import cross_val_score\n","from sklearn.svm import SVR\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Lasso, Ridge, ElasticNet\n","import numpy as np\n","\n","def get_model_cv_prediction():\n","    # 모델 이름과 모델 클래스 정의\n","    models = {\n","        'dt': DecisionTreeRegressor,\n","        'rf': RandomForestRegressor,\n","        'gb': GradientBoostingRegressor,\n","        'lr': LinearRegression,\n","        'svr': SVR,\n","        'xgb': XGBRegressor,\n","        'lgb': LGBMRegressor,\n","        'rr': Ridge,\n","        'lasso': Lasso,\n","        'elasticnet': ElasticNet\n","    }\n","\n","    # 모델 하이퍼파라미터 설정\n","    params = {\n","        'dt': {'random_state': 0, 'max_depth': 4},\n","        'rf': {'random_state': 0, 'n_estimators': 1000},\n","        'gb': {'random_state': 0, 'n_estimators': 1000},\n","        'lr': {},\n","        'svr': {'kernel': 'linear', 'C': 1.0},\n","        'xgb': {'n_estimators': 1000},\n","        'lgb': {'n_estimators': 1000, 'verbose': -1},\n","        'rr': {'alpha': 0.1},\n","        'lasso': {'alpha': 0.1},\n","        'elasticnet': {'alpha': 0.1, 'l1_ratio': 0.5}\n","    }\n","\n","    # 모델 객체 생성\n","    model_objects = {}\n","    for model_name, model_class in models.items():\n","        model_objects[model_name] = model_class(**params[model_name])\n","\n","    return model_objects\n","\n","def evaluate_models(X_data, y_target):\n","    model_dict = get_model_cv_prediction()\n","\n","    for name, model in model_dict.items():\n","        # 교차 검증을 통해 모델 평가\n","        neg_mse_scores = cross_val_score(model, X_data, y_target, scoring='neg_mean_squared_error', cv=5)\n","        rmse_scores = np.sqrt(-neg_mse_scores)\n","        avg_rmse = np.mean(rmse_scores)\n","\n","        # 결과 출력\n","        print(f'{name} 모델의 5 folds의 개별 Negative MSE scores: {np.round(neg_mse_scores, 3)}')\n","        print(f'{name} 모델의 5 folds의 개별 RMSE scores : {np.round(rmse_scores, 3)}')\n","        print(f'{name} 모델의 5 folds의 평균 RMSE scores : {avg_rmse:.3f}\\n')\n","\n","evaluate_models(X_train_scaled, y_train)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FERd3grKPwdX","executionInfo":{"status":"ok","timestamp":1722406642522,"user_tz":-540,"elapsed":1029699,"user":{"displayName":"이현석","userId":"06775013589116224529"}},"outputId":"f03f24cf-7284-440f-a601-ba2cb0a47ba4"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["dt 모델의 5 folds의 개별 Negative MSE scores: [-0.591 -0.539 -0.561 -0.574 -0.59 ]\n","dt 모델의 5 folds의 개별 RMSE scores : [0.769 0.734 0.749 0.758 0.768]\n","dt 모델의 5 folds의 평균 RMSE scores : 0.756\n","\n","rf 모델의 5 folds의 개별 Negative MSE scores: [-0.26  -0.264 -0.254 -0.252 -0.262]\n","rf 모델의 5 folds의 개별 RMSE scores : [0.51  0.514 0.504 0.502 0.512]\n","rf 모델의 5 folds의 평균 RMSE scores : 0.508\n","\n","gb 모델의 5 folds의 개별 Negative MSE scores: [-0.224 -0.224 -0.231 -0.221 -0.236]\n","gb 모델의 5 folds의 개별 RMSE scores : [0.473 0.473 0.48  0.47  0.486]\n","gb 모델의 5 folds의 평균 RMSE scores : 0.477\n","\n","lr 모델의 5 folds의 개별 Negative MSE scores: [-0.52  -0.502 -0.521 -0.508 -0.546]\n","lr 모델의 5 folds의 개별 RMSE scores : [0.721 0.709 0.721 0.713 0.739]\n","lr 모델의 5 folds의 평균 RMSE scores : 0.721\n","\n","svr 모델의 5 folds의 개별 Negative MSE scores: [-0.54  -0.528 -6.042 -0.924 -0.808]\n","svr 모델의 5 folds의 개별 RMSE scores : [0.735 0.726 2.458 0.961 0.899]\n","svr 모델의 5 folds의 평균 RMSE scores : 1.156\n","\n","xgb 모델의 5 folds의 개별 Negative MSE scores: [-0.23  -0.241 -0.23  -0.22  -0.228]\n","xgb 모델의 5 folds의 개별 RMSE scores : [0.479 0.491 0.479 0.469 0.477]\n","xgb 모델의 5 folds의 평균 RMSE scores : 0.479\n","\n","lgb 모델의 5 folds의 개별 Negative MSE scores: [-0.204 -0.222 -0.202 -0.206 -0.21 ]\n","lgb 모델의 5 folds의 개별 RMSE scores : [0.451 0.471 0.449 0.453 0.458]\n","lgb 모델의 5 folds의 평균 RMSE scores : 0.457\n","\n","rr 모델의 5 folds의 개별 Negative MSE scores: [-0.52  -0.502 -0.521 -0.508 -0.546]\n","rr 모델의 5 folds의 개별 RMSE scores : [0.721 0.709 0.721 0.713 0.739]\n","rr 모델의 5 folds의 평균 RMSE scores : 0.721\n","\n","lasso 모델의 5 folds의 개별 Negative MSE scores: [-0.684 -0.648 -0.681 -0.665 -0.683]\n","lasso 모델의 5 folds의 개별 RMSE scores : [0.827 0.805 0.825 0.815 0.826]\n","lasso 모델의 5 folds의 평균 RMSE scores : 0.820\n","\n","elasticnet 모델의 5 folds의 개별 Negative MSE scores: [-0.636 -0.606 -0.637 -0.618 -0.64 ]\n","elasticnet 모델의 5 folds의 개별 RMSE scores : [0.797 0.779 0.798 0.786 0.8  ]\n","elasticnet 모델의 5 folds의 평균 RMSE scores : 0.792\n","\n"]}]},{"cell_type":"markdown","source":["- GradientBoosting / XGB / LGBM 의 값이\\\n","상대적으로 가장 낮은 것으로 보아 그나마 성능이 좋은 모델인 것으로 추측\n","\n"],"metadata":{"id":"rqOlHd37fB_9"}},{"cell_type":"code","source":["gb, xgb, lgb = GradientBoostingRegressor(), XGBRegressor(), LGBMRegressor()\n","gb.fit(X_train_scaled, y_train)\n","xgb.fit(X_train_scaled, y_train)\n","lgb.fit(X_train_scaled, y_train)\n","\n","\n","# Lasso Regression에 GridSearchCV 적용\n","\n","# alpha : 0.001 ~ 100 / 10배수 단위로 적용\n","# fit_intercept : False or True / 기본값 : True / 모델에 절편 포함 여부 결정\n","# precompute : X^TX를 미리 계산하여 성능 개선 / 큰 데이터셋에서는 사용 불가 / 기본값 : False\n","# max_iter : 반복 횟수 / 기본값 : 1000\n","# positive : 회귀 계수가 양수로 제한 / 기본값 : False\n","# random_state\n","\n","# GridSearchCV를 사용한 하이퍼파라미터 튜닝\n","grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n","grid_search.fit(X_train, y_train)"],"metadata":{"id":"rm4s-Y4aUDQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","import numpy as np\n","\n","# 각 모델의 하이퍼파라미터 그리드 설정\n","param_grids = {\n","     'gb': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.1, 1],\n","        'max_depth': [3, 5, 7]\n","    },\n","    'xgb': {\n","        'n_estimators': [100, 200, 500],\n","        'learning_rate': [0.01, 0.1, 1],\n","        'max_depth': [3, 5, 7]\n","    },\n","    'lgb': {\n","        'n_estimators': [100, 200, 500, 1000],\n","        'learning_rate': [0.01, 0.1, 1],\n","        'max_depth': [7, 10]\n","    }\n","}\n","\n","# 모델 객체 생성\n","models = {\n","    'gb': GradientBoostingRegressor(),\n","    'xgb': XGBRegressor(),\n","    'lgb': LGBMRegressor()\n","}\n","\n","# 하이퍼파라미터 튜닝 및 모델 평가\n","for model_name, model in models.items():\n","    print(f\"Evaluating {model_name}...\")\n","\n","    grid_search = GridSearchCV(model, param_grids[model_name], cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n","    grid_search.fit(X_train, y_train)\n","\n","    # 최적의 하이퍼파라미터 출력\n","    print(f\"Best Parameters for {model_name}:\", grid_search.best_params_)\n","\n","    # 최적의 모델을 사용하여 테스트 데이터 평가\n","    best_model = grid_search.best_estimator_\n","    y_pred = best_model.predict(X_test)\n","\n","    # 평가 사용자 함수 정의\n","    def evaluate_model(y_pred, y_test):\n","        # RMSE 계산\n","        rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n","\n","        # 결과 출력\n","        print(\"RMSE:\", rmse)\n","\n","    evaluate_model(y_pred, y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0NvMJQWesLY","executionInfo":{"status":"ok","timestamp":1722410857911,"user_tz":-540,"elapsed":2103658,"user":{"displayName":"이현석","userId":"06775013589116224529"}},"outputId":"efc804c8-d21a-46e9-e520-634c54d9feab"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating gb...\n","Best Parameters for gb: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}\n","RMSE: 0.4559462197976954\n","Evaluating xgb...\n","Best Parameters for xgb: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}\n","RMSE: 0.45547070284167057\n","Evaluating lgb...\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001395 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 1838\n","[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n","[LightGBM] [Info] Start training from score 2.071947\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Best Parameters for lgb: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500}\n","RMSE: 0.43522972934099036\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","import numpy as np\n","\n","# 각 모델의 하이퍼파라미터 그리드 설정\n","param_grids = {\n","#     'gb': {\n","#        'n_estimators': [100, 200, 500],\n","#        'learning_rate': [0.01, 0.1, 1],\n","#        'max_depth': [3, 5, 7]\n","#    },\n","#    'xgb': {\n","#        'n_estimators': [100, 200, 500],\n","#        'learning_rate': [0.01, 0.1, 1],\n","#        'max_depth': [3, 5, 7]\n","#    },\n","    'lgb': {\n","        'n_estimators': [200, 500, 1000],\n","        'learning_rate': [0.01, 0.1, 1],\n","        'max_depth': [7, 10],\n","        'min_child_samples': [20, 50, 100],\n","        'min_child_weight': [0.1, 1, 10],\n","        'min_gain_to_split': [0.0, 0.1, 0.2]\n","    }\n","}\n","\n","# 모델 객체 생성\n","models = {\n","#    'gb': GradientBoostingRegressor(),\n","#    'xgb': XGBRegressor(),\n","    'lgb': LGBMRegressor()\n","}\n","\n","# 하이퍼파라미터 튜닝 및 모델 평가\n","for model_name, model in models.items():\n","    print(f\"Evaluating {model_name}...\")\n","\n","    grid_search = GridSearchCV(model, param_grids[model_name], cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n","    grid_search.fit(X_train, y_train)\n","\n","    # 최적의 하이퍼파라미터 출력\n","    print(f\"Best Parameters for {model_name}:\", grid_search.best_params_)\n","\n","    # 최적의 모델을 사용하여 테스트 데이터 평가\n","    best_model = grid_search.best_estimator_\n","    y_pred = best_model.predict(X_test)\n","\n","    # 평가 사용자 함수 정의\n","    def evaluate_model(y_pred, y_test):\n","        # RMSE 계산\n","        rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n","\n","        # 결과 출력\n","        print(\"RMSE:\", rmse)\n","\n","    evaluate_model(y_pred, y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4CHvAlUscDG","executionInfo":{"status":"ok","timestamp":1722413405862,"user_tz":-540,"elapsed":2161592,"user":{"displayName":"이현석","userId":"06775013589116224529"}},"outputId":"f1c48617-ab87-4026-f907-0cb92b6c4f7b"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating lgb...\n","[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n","[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002651 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 1838\n","[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n","[LightGBM] [Info] Start training from score 2.071947\n","Best Parameters for lgb: {'learning_rate': 0.1, 'max_depth': 10, 'min_child_samples': 20, 'min_child_weight': 0.1, 'min_gain_to_split': 0.0, 'n_estimators': 500}\n","[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\n","RMSE: 0.4378896185939582\n"]}]},{"cell_type":"markdown","source":["- GradingBoosting : 'learning_rate':0.1, 'max_depth':5, 'n_estimators:500 → 0.455\n","- XGB : 'learning_rate':0.1, 'max_depth':5, 'n_estimators:500 → 0.455\n","- LGBM : 'learning_rate': 0.1, 'max_depth':10, 'n_estimators:500 → 0.435"],"metadata":{"id":"9OAJQ9-MrqeN"}},{"cell_type":"code","source":[],"metadata":{"id":"EJHwfqESvX1f"},"execution_count":null,"outputs":[]}]}